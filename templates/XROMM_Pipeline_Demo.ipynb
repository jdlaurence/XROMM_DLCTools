{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RK255E7YoEIt"
   },
   "source": [
    "# DeepLabCut + XROMM Toolbox\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "Nath*, Mathis* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n",
    "\n",
    "Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
    "\n",
    "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n",
    "\n",
    "\n",
    "\n",
    "XROMMTools for DeepLabCut written by J.D. Laurence-Chasen\n",
    "\n",
    "This notebook walks you through the XMALab/DeepLabCut workflow, from project creation to analysis.\n",
    "\n",
    "At this stage, you should have tracked data in XMALab and exported the 2D points for trials you wish to be included in the initial training dataset. Each trial should have its own subfolder, containing **only** the images (image stack or video files) and the 2D points CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import packages. Run this every time!\n",
    "import deeplabcut\n",
    "from deeplabcut.utils import xrommtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Uoz9mdPoEIy"
   },
   "source": [
    "## Create a new project\n",
    "\n",
    "The first step is to create a DeepLabCut project (network). Whether or not you create new projects for each day of data collection depends in large part on the variation in the dataset and number of markers. You should always create new projects for different individuals.\n",
    "\n",
    "N.B. If you are creating a new project for a different day of data collection on the same animal (or an animal with a similar marker scheme), it may be easier to manually copy an existing project and delete the folder contents to minimize config file editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS SECTION ONLY ONCE, when you first create the project. \n",
    "# If a symbolic link error occurs, make sure you've run Anaconda Navigator \"as Administrator\"\n",
    "\n",
    "task='PigFeeding' # Enter the name of your experiment Task\n",
    "experimenter='JD' # Enter the name of the experimenter\n",
    "video=['C:/Users/jdlc7/PigDLC/Pig_20191203.avi'] # IMPORTANT: This video is a 'dummy' video. Just point the function to something.\n",
    "working_directory = 'C:/Users/jdlc7/Pig_20191203' # where the project will be created; use forward slashes on Windows\n",
    "\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video,working_directory,copy_videos=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Uoz9mdPoEIy"
   },
   "source": [
    "## Now edit the newly created configuration file to match your project. \n",
    "\n",
    "See DLC methods paper for the significance of the different terms.\n",
    "\n",
    "Make sure the body parts correspond **EXACTLY** to your point names/order in XMALab (this is why consistent naming is important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory paths if you haven't yet. This is is for when you return to this notebook (so you don't re-run the cell above).\n",
    "\n",
    "path_config_file = 'C:/Users/jdlc7/PigDLC/PigFeeding-JD-2019-12-03/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Uoz9mdPoEIy"
   },
   "source": [
    "## Convert XMALab 2D points to DLC format\n",
    "\n",
    "Now we extract and convert the 2D points file/s and frames from the videos. The function xma_to_dlc will populate the project's labeled-data folder with the extracted data, such that it's ready for DeepLabCut's native create_training_dataset function.\n",
    "\n",
    "**Be sure** that dataset_name exactly matches the dummy video under \"video sets\" in the config file (but without the .avi).\n",
    "So if your dummy video in the config file is '.../Pig_20191203.avi', then dataset name must be 'Pig_20191203'.\n",
    "\n",
    "**Also**, be sure your 2D points files don't contain any leading or trailing spaces.\n",
    "You can get rid of those using Excel's trim() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9DjG55FoEI7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experimenter = 'JD'\n",
    "data_path = 'C:/Users/jdlc7/Desktop/Pig_20191203/DLC' # where are the trial folders (with 2D points/trial images)\n",
    "dataset_name = 'Pig_20191203'\n",
    "nframes = 400 # how many frames do you want the training dataset to comprise? Make sure it's equal to or less than the total numbed of tracked frames\n",
    "\n",
    "xrommtools.xma_to_dlc(path_config_file,data_path,dataset_name,experimenter,nframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNi9s1dboEJN"
   },
   "source": [
    "## Create a training dataset\n",
    "\n",
    "This function generates the training data information for network training based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles if they want to benchmark the performance (typically, 1 is what you will set, so you pass nothing!). \n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n",
    "\n",
    "This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**. For most all use cases we have seen, the defaults are perfectly fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMeUwgxPoEJP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4FczXGDoEJU"
   },
   "source": [
    "## Start training:\n",
    "\n",
    "This function trains the network for a specific shuffle of the training dataset. This will take a long time. You can stop training manually by hitting stop. It will look like an error has occured...it has not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pOvDq_2oEJW"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file) # there are many other parameters you can set here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xZygsb2DoEJc"
   },
   "source": [
    "## Evaluating Network\n",
    "\n",
    "This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
    "and stores the results as .csv file in a subdirectory under **evaluation-results**. The values here can give you an indication of performance/generalizability, but are less informative than reprojection error and other XMALab metrics. It's recommended you use this to assess whether something went really wrong (i.e. errors higher than 2-3 pixels.), but importing predicted points into XMALab is the best way to assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nv4zlbrnoEJg"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVFLSKKfoEJk"
   },
   "source": [
    "## Analyzing videos\n",
    "\n",
    "We call the existing DLC function analyze_videos to predict points for new trials and convert the output to XMALab format. The results are stored in hd5 file and CSV file in each trial's specific iteration subfolder.\n",
    "\n",
    "The base function analyze_videos has many possible arguments. To alter this, you will need to edit the xrommtools script manually. Its default form should be sufficient in most cases.\n",
    "\n",
    "\n",
    "**Note, this has been tested only on .avi files**. If you have jpg stacks (which is fine for creating or augmenting the training dataset), we recommend you convert to .avi if you want to predict points here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_LZiS_0oEJl"
   },
   "outputs": [],
   "source": [
    "new_data_path = 'C:/Users/jdlc7/Desktop/Pig_20191203/Trials' # where are the trials you wish to analyze? \n",
    "# IMPORTANT: new_data_path must only contain trial folders, which in turn must only contain video files.\n",
    "# The function automatically reads camera 1 and camera 2 from the file name, thus any other files/folders will cause an error.\n",
    "\n",
    "iteration = 0 # what iteration of the network do you want to analyze the videos?\n",
    "#Start at 0, and if you augment and retrain, update this as well as the config file\n",
    "\n",
    "xrommtools.analyze_xromm_videos(path_config_file, new_data_path, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Augment the training dataset with corrected frames [optional step]\n",
    "\n",
    "If performance isn't adaqute (i.e. high reprojection errors), you can correct frames in XMALab and add them to the training dataset, and re-train iteratively. \n",
    "\n",
    "Following the correction of frames in XMALab and the re-exporting of 2D points, you can augment the training dataset and retrain the networks. \n",
    "\n",
    "**Make sure** the newley exported 2D points files have some form of 'corrected' in their file names, and that they are located in the itX subfolder of each trial.\n",
    "\n",
    "Correct formatting of the frames file is essential, so refer to the instructions to make sure you have it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytQoxIldoEJ7"
   },
   "outputs": [],
   "source": [
    "frames = 'C:/Users/jdlc7/PigDLC/frameindex.csv' # column 1 should be trialnames, and columns 2+ should be frame numbers\n",
    "\n",
    "new_data_path = 'C:/Users/jdlc7/Desktop/Pig_20191203/Trials' # where are trial folders with new videos to analyze?\n",
    "#This should be the same as new_data_path for analyzing videos\n",
    "\n",
    "iteration = 0 # What iteration subfolder should the function look for the corrected 2D points files in?\n",
    "\n",
    "xrommtools.add_frames(path_config_file, new_data_path, iteration, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCHj7qyboEJ6"
   },
   "source": [
    "## Create a new iteration of training dataset\n",
    "\n",
    "Once you've added frames to the training dataset using the add_frames function, you can now use the native function once again to create a new iteration of the training dataset.\n",
    "\n",
    "**It is essential** that at this point you update the iteration in the config file, i.e. 0 --> 1 after the first augmentation.\n",
    "\n",
    "After you re-create the training dataset, you can re-run train_network and repeat all the following steps until performance is acceptable (or it plateaus).",
    "\n",
    "If you want to resume training with the existing weights, rather than starting from scractch, be sure to edit init_weights in the pose_config.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCrUvQIvoEKD"
   },
   "source": [
    "## Create labeled video\n",
    "This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. \n",
    "\n",
    "THIS HAS MANY FUN OPTIONS! \n",
    "\n",
    "``deeplabcut.create_labeled_video(config, videos, videotype='avi', shuffle=1, trainingsetindex=0, filtered=False, save_frames=False, Frames2plot=None, delete=False, displayedbodyparts='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False)``\n",
    "\n",
    "So please check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aDF7Q7KoEKE"
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GTiuJESoEKH"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX21zZbXoEKJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook #for making interactive plots.\n",
    "deeplabcut.plot_trajectories(path_config_file,videofile_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-yourowndata.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
